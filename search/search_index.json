{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Current situation and learning path Goal Transition into a data engineering role. Data Engineering What is it? What skills are needed? https://awesomedataengineering.com/ SQL Python At least 1 cloud computing platform Relational Databases 1 pipeline management tool / platform Current competencies SQL - intermediate Python for data analysis - Pandas. Working knowledge of GCP tools: - BigQuery - Cloud Functions - App Engine - DataPrep - Google Cloud Storage skill gaps identified Advanced SQL skills. specifically around creating and modifying data. Stored procedures. GCP - data engineering certificate. Python Airflow Data warehousing principles. Roadmap 1. Google Cloud platform certification On tools training through qwiklabs Review of GCP documentation Review of exam materials exam course guide pass data engineering certification exam 2. Level up SQL 3. Python Specific for data engineering Work through \"Data Engineering with Python\" book . 4. Data warehousing Work through Kimball data warehousing book . 5. Projects Create serverless web analytics functionality Create cryptocurrency dashboard Review commercial ETL tools such as: Stitch data, Panoply, FiveTran, Airbyte","title":"Current situation and learning path"},{"location":"#current-situation-and-learning-path","text":"","title":"Current situation and learning path"},{"location":"#goal","text":"Transition into a data engineering role.","title":"Goal"},{"location":"#data-engineering","text":"","title":"Data Engineering"},{"location":"#what-is-it","text":"","title":"What is it?"},{"location":"#what-skills-are-needed","text":"https://awesomedataengineering.com/ SQL Python At least 1 cloud computing platform Relational Databases 1 pipeline management tool / platform","title":"What skills are needed?"},{"location":"#current-competencies","text":"SQL - intermediate Python for data analysis - Pandas. Working knowledge of GCP tools: - BigQuery - Cloud Functions - App Engine - DataPrep - Google Cloud Storage","title":"Current competencies"},{"location":"#skill-gaps-identified","text":"Advanced SQL skills. specifically around creating and modifying data. Stored procedures. GCP - data engineering certificate. Python Airflow Data warehousing principles.","title":"skill gaps identified"},{"location":"#roadmap","text":"","title":"Roadmap"},{"location":"#1-google-cloud-platform-certification","text":"On tools training through qwiklabs Review of GCP documentation Review of exam materials exam course guide pass data engineering certification exam","title":"1. Google Cloud platform certification"},{"location":"#2-level-up-sql","text":"","title":"2. Level up SQL"},{"location":"#3-python-specific-for-data-engineering","text":"Work through \"Data Engineering with Python\" book .","title":"3. Python Specific for data engineering"},{"location":"#4-data-warehousing","text":"Work through Kimball data warehousing book .","title":"4. Data warehousing"},{"location":"#5-projects","text":"Create serverless web analytics functionality Create cryptocurrency dashboard Review commercial ETL tools such as: Stitch data, Panoply, FiveTran, Airbyte","title":"5. Projects"},{"location":"Data%20Engineering%20with%20python/Chapter%201/","text":"Chapter 1: What is data engineering What data engineers do Typical tasks include extracting, loading, and transforming data. Data engineers Query from a source (extract) Perform some modifications (transform) put the data where others can access it now it is production quality (load) ETL! Example A retailer that has a transactional database in each region it operates. to answer questions about total sales data from all databases would be required (in a single place) ... but not before some transformations are applied, to standardise the timestamps across each region. The combination of extracting, loading, and transforming data is accomplished by the creation of a data pipeline The data comes into the pipeline raw, or dirty in the sense that there may be missing data or typos in the data, which is then cleaned as it flows through the pipe. After that, it comes out the other side into a data warehouse, where it can be queried. The following diagram shows the pipeline required to accomplish the task. Required skills and knowledge to be a data engineer Data engineers typically need to know multiple programming languages such as Python and SQL. Data modeling a structures knowledge is important at the transformation stage. As well as data warehouse design. Data engineering scope may also include the infrastructure on which the data pipelines run. Eg linux servers, or the corresponding tools on cloud platforms. Data engineering is the development, operation, and maintenance of data infrastructure, either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases and pipelines to extract, transform, and load data. Programming languages SQL is the main language of data engineering. Java and Scala are widely used in data engineering tools. Our focus will be on Python. Databases Most data in production systems is stored in relational databases. Examples include: Oracle, Microsoft SWL Server, MySQL, and PostgreSQL. Popular choices for data warehouses include: Amazon Redshift Google BigQuery Apache Cassandra Elasticsearch Data Processing Engines Once a data engineer extracts data from a database, they will need to transform or process it. With big data, it helps to use a data processing engine. The most popular engine is Apache Spark. Data Pipelines ETL processes will likely need to be run on a shcedule. Crontab was the original scheduler. However additional complexities such as monitoring sucess and failures, and what ran and what didn't leads to a need for a better framework. Apache Airflow Apache Airflow is the most popular data pipeline framework in Python. Airflow has the following built in: web server scheduler metastore queueing system executors Airflow uses Directed Acyclic Graphs (DAGs) Airflow can be run on a single machine, or on a cluster with nodes. Apache NiFi Apache NiFi is a framework for building data engineering pipelines.","title":"Chapter 1: What is data engineering"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#chapter-1-what-is-data-engineering","text":"","title":"Chapter 1: What is data engineering"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#what-data-engineers-do","text":"Typical tasks include extracting, loading, and transforming data. Data engineers Query from a source (extract) Perform some modifications (transform) put the data where others can access it now it is production quality (load) ETL! Example A retailer that has a transactional database in each region it operates. to answer questions about total sales data from all databases would be required (in a single place) ... but not before some transformations are applied, to standardise the timestamps across each region. The combination of extracting, loading, and transforming data is accomplished by the creation of a data pipeline The data comes into the pipeline raw, or dirty in the sense that there may be missing data or typos in the data, which is then cleaned as it flows through the pipe. After that, it comes out the other side into a data warehouse, where it can be queried. The following diagram shows the pipeline required to accomplish the task.","title":"What data engineers do"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#required-skills-and-knowledge-to-be-a-data-engineer","text":"Data engineers typically need to know multiple programming languages such as Python and SQL. Data modeling a structures knowledge is important at the transformation stage. As well as data warehouse design. Data engineering scope may also include the infrastructure on which the data pipelines run. Eg linux servers, or the corresponding tools on cloud platforms. Data engineering is the development, operation, and maintenance of data infrastructure, either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases and pipelines to extract, transform, and load data.","title":"Required skills and knowledge to be a data engineer"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#programming-languages","text":"SQL is the main language of data engineering. Java and Scala are widely used in data engineering tools. Our focus will be on Python.","title":"Programming languages"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#databases","text":"Most data in production systems is stored in relational databases. Examples include: Oracle, Microsoft SWL Server, MySQL, and PostgreSQL. Popular choices for data warehouses include: Amazon Redshift Google BigQuery Apache Cassandra Elasticsearch","title":"Databases"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#data-processing-engines","text":"Once a data engineer extracts data from a database, they will need to transform or process it. With big data, it helps to use a data processing engine. The most popular engine is Apache Spark.","title":"Data Processing Engines"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#data-pipelines","text":"ETL processes will likely need to be run on a shcedule. Crontab was the original scheduler. However additional complexities such as monitoring sucess and failures, and what ran and what didn't leads to a need for a better framework.","title":"Data Pipelines"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#apache-airflow","text":"Apache Airflow is the most popular data pipeline framework in Python. Airflow has the following built in: web server scheduler metastore queueing system executors Airflow uses Directed Acyclic Graphs (DAGs) Airflow can be run on a single machine, or on a cluster with nodes.","title":"Apache Airflow"},{"location":"Data%20Engineering%20with%20python/Chapter%201/#apache-nifi","text":"Apache NiFi is a framework for building data engineering pipelines.","title":"Apache NiFi"},{"location":"Data%20Engineering%20with%20python/Chapter%202/","text":"Chapter 2: Building Our Data Engineering Infrastructure Installing and configuring the following tools: Apache NiFi Apache Airflow ElasticSearch Kibana PostgreSQL Installing Apache NiFi # Download NiFi curl 'https://archive.apache.org/dist/nifi/1.12.1/nifi-1.12.1-bin.tar.gz' --output 'nifi.tar.gz' # Extract files from downloaded archive tar xvzf 'nifi.tar.gz' # if necessary install Java sudo apt install openjdk-11-jre-headless","title":"Chapter 2: Building Our Data Engineering Infrastructure"},{"location":"Data%20Engineering%20with%20python/Chapter%202/#chapter-2-building-our-data-engineering-infrastructure","text":"Installing and configuring the following tools: Apache NiFi Apache Airflow ElasticSearch Kibana PostgreSQL","title":"Chapter 2: Building Our Data Engineering Infrastructure"},{"location":"Data%20Engineering%20with%20python/Chapter%202/#installing-apache-nifi","text":"# Download NiFi curl 'https://archive.apache.org/dist/nifi/1.12.1/nifi-1.12.1-bin.tar.gz' --output 'nifi.tar.gz' # Extract files from downloaded archive tar xvzf 'nifi.tar.gz' # if necessary install Java sudo apt install openjdk-11-jre-headless","title":"Installing Apache NiFi"},{"location":"Data%20Engineering%20with%20python/misc/","text":"[https://github.com/PacktPublishing/Data-Engineering-with-Python])https://github.com/PacktPublishing/Data-Engineering-with-Python)","title":"Misc"},{"location":"Databases/Database_design/","text":"Database Design video course and notes Data is stored about entities. Entities have attributes. The attributes are stored in database tables. rows contain specific attributes for a given entity. columns contain a list of the same attribute but for different entities eg usernames. Entity type - Category of entity eg User, a specific entity would be paul. Attibute type - Category of attribute eg username (R)DBMS (Relational) Database management system DBMS allows data to be viewed and queried and manipulated. Views can be created to allow access to different fields to the data. examples of RDBMS include - MySQL - SQL server - ORACLE - PostgreSQL Data for a database is stored on disk. The database management system coordinates all the data moving in off disk and associating it with the correct table. SQL Structured Queried Language All databases use a version of SQL. SQL is used to define database structure, and then manipulate the data within. DDL - data definition language DML - data manipulation language 40mins","title":"Database Design video course and notes"},{"location":"Databases/Database_design/#database-design-video-course-and-notes","text":"Data is stored about entities. Entities have attributes. The attributes are stored in database tables. rows contain specific attributes for a given entity. columns contain a list of the same attribute but for different entities eg usernames. Entity type - Category of entity eg User, a specific entity would be paul. Attibute type - Category of attribute eg username","title":"Database Design video course and notes"},{"location":"Databases/Database_design/#rdbms","text":"(Relational) Database management system DBMS allows data to be viewed and queried and manipulated. Views can be created to allow access to different fields to the data. examples of RDBMS include - MySQL - SQL server - ORACLE - PostgreSQL Data for a database is stored on disk. The database management system coordinates all the data moving in off disk and associating it with the correct table.","title":"(R)DBMS"},{"location":"Databases/Database_design/#sql","text":"Structured Queried Language All databases use a version of SQL. SQL is used to define database structure, and then manipulate the data within. DDL - data definition language DML - data manipulation language 40mins","title":"SQL"},{"location":"Decision%20Journal/01%20GCP%20decision/","text":"Decision: Choice of cloud platform to learn Decision Number 1 Date 1st March 2021 Time pm Decision Which cloud service to learn to pivot into a data engineering role. Mental / Physical State [ ] Energised [ ] Focussed [x] Relaxed [ ] Confident [ ] Tired [ ] Accepting [ ] Accomodating [ ] Anxious [ ] Resigned [ ] Frustrated [ ] Angry The Situation / context Modern data engineers use cloud platforms to perform tasks required. Familiarity with at least 1 cloud platform is required to enter a data engineering role. Notable Cloud platforms include: Amazon Web Services Azure (Microsoft) Google Cloud Platform Oracle Alibaba I am already familiar with various aspects of Google Cloud platform. The problem statement or frame There are multiple cloud platforms that could be employed for data engineering. Becoming familiar with all of them is not the best use of time. It is necessary to choose a single Cloud platform to become proficcient in. Decision Learn how to perform key data engineering tasks using Google Cloud Platform. The Variables that govern the situation include AWS has the largest market share Globally and within Australia. Learning AWS will provide the greatest optionality for future opportunities. I have some experience with Google Cloud Platform. Woolworths (current employer) have made a company wide decision that the cloud platform of choice will be Google Cloud Platform. If I want to transition to data engineer within Woolworths I will need to be competent with GCP. Woolworths will provide training materials to become proficient on GCP. Alternatives that were seriously considered and not chosen were AWS and Azure Explain the range of outcomes Best case transition into a data engineering role within current employer Other cases Transition into a data engineering role outside current employer with GCP skills Fail to transition into a data engineering role within current employer with GCP skills Fail to transition to data engineering role outside of current employer with GCP skills What I expect to happen and the actual probabilities are Best case is medium to high probability (75%) given the following are met: GCP data engineer certification Demonstrate capability with Cloud composer, data flow, Google Cloud functions, Big Query. The outcome TBC","title":"Decision: Choice of cloud platform to learn"},{"location":"Decision%20Journal/01%20GCP%20decision/#decision-choice-of-cloud-platform-to-learn","text":"","title":"Decision: Choice of cloud platform to learn"},{"location":"Decision%20Journal/01%20GCP%20decision/#decision-number","text":"1","title":"Decision Number"},{"location":"Decision%20Journal/01%20GCP%20decision/#date","text":"1st March 2021","title":"Date"},{"location":"Decision%20Journal/01%20GCP%20decision/#time","text":"pm","title":"Time"},{"location":"Decision%20Journal/01%20GCP%20decision/#decision","text":"Which cloud service to learn to pivot into a data engineering role.","title":"Decision"},{"location":"Decision%20Journal/01%20GCP%20decision/#mental-physical-state","text":"[ ] Energised [ ] Focussed [x] Relaxed [ ] Confident [ ] Tired [ ] Accepting [ ] Accomodating [ ] Anxious [ ] Resigned [ ] Frustrated [ ] Angry","title":"Mental / Physical State"},{"location":"Decision%20Journal/01%20GCP%20decision/#the-situation-context","text":"Modern data engineers use cloud platforms to perform tasks required. Familiarity with at least 1 cloud platform is required to enter a data engineering role. Notable Cloud platforms include: Amazon Web Services Azure (Microsoft) Google Cloud Platform Oracle Alibaba I am already familiar with various aspects of Google Cloud platform.","title":"The Situation / context"},{"location":"Decision%20Journal/01%20GCP%20decision/#the-problem-statement-or-frame","text":"There are multiple cloud platforms that could be employed for data engineering. Becoming familiar with all of them is not the best use of time. It is necessary to choose a single Cloud platform to become proficcient in. Decision Learn how to perform key data engineering tasks using Google Cloud Platform.","title":"The problem statement or frame"},{"location":"Decision%20Journal/01%20GCP%20decision/#the-variables-that-govern-the-situation-include","text":"AWS has the largest market share Globally and within Australia. Learning AWS will provide the greatest optionality for future opportunities. I have some experience with Google Cloud Platform. Woolworths (current employer) have made a company wide decision that the cloud platform of choice will be Google Cloud Platform. If I want to transition to data engineer within Woolworths I will need to be competent with GCP. Woolworths will provide training materials to become proficient on GCP.","title":"The Variables that govern the situation include"},{"location":"Decision%20Journal/01%20GCP%20decision/#alternatives-that-were-seriously-considered-and-not-chosen-were","text":"AWS and Azure","title":"Alternatives that were seriously considered and not chosen were"},{"location":"Decision%20Journal/01%20GCP%20decision/#explain-the-range-of-outcomes","text":"Best case transition into a data engineering role within current employer Other cases Transition into a data engineering role outside current employer with GCP skills Fail to transition into a data engineering role within current employer with GCP skills Fail to transition to data engineering role outside of current employer with GCP skills","title":"Explain the range of outcomes"},{"location":"Decision%20Journal/01%20GCP%20decision/#what-i-expect-to-happen-and-the-actual-probabilities-are","text":"Best case is medium to high probability (75%) given the following are met: GCP data engineer certification Demonstrate capability with Cloud composer, data flow, Google Cloud functions, Big Query.","title":"What I expect to happen and the actual probabilities are"},{"location":"Decision%20Journal/01%20GCP%20decision/#the-outcome","text":"TBC","title":"The outcome"},{"location":"Decision%20Journal/dj%20template/","text":"Decision Journal Template Inspiration : https://fs.blog/2014/02/decision-journal/ Decision Number Date Time Decision Mental / Physical State [ ] Energised [ ] Focussed [x] Relaxed [ ] Confident [ ] Tired [ ] Accepting [ ] Accomodating [ ] Anxious [ ] Resigned [ ] Frustrated [ ] Angry The Situation / context The problem statement or frame The Variables that govern the situation include The complications/complexities as I see them Alternatives that were seriously considered and not chosen were Explain the range of outcomes What I expect to happen and the actual probabilities are The outcome","title":"Decision Journal Template"},{"location":"Decision%20Journal/dj%20template/#decision-journal-template","text":"Inspiration : https://fs.blog/2014/02/decision-journal/","title":"Decision Journal Template"},{"location":"Decision%20Journal/dj%20template/#decision-number","text":"","title":"Decision Number"},{"location":"Decision%20Journal/dj%20template/#date","text":"","title":"Date"},{"location":"Decision%20Journal/dj%20template/#time","text":"","title":"Time"},{"location":"Decision%20Journal/dj%20template/#decision","text":"","title":"Decision"},{"location":"Decision%20Journal/dj%20template/#mental-physical-state","text":"[ ] Energised [ ] Focussed [x] Relaxed [ ] Confident [ ] Tired [ ] Accepting [ ] Accomodating [ ] Anxious [ ] Resigned [ ] Frustrated [ ] Angry","title":"Mental / Physical State"},{"location":"Decision%20Journal/dj%20template/#the-situation-context","text":"","title":"The Situation / context"},{"location":"Decision%20Journal/dj%20template/#the-problem-statement-or-frame","text":"","title":"The problem statement or frame"},{"location":"Decision%20Journal/dj%20template/#the-variables-that-govern-the-situation-include","text":"","title":"The Variables that govern the situation include"},{"location":"Decision%20Journal/dj%20template/#the-complicationscomplexities-as-i-see-them","text":"","title":"The complications/complexities as I see them"},{"location":"Decision%20Journal/dj%20template/#alternatives-that-were-seriously-considered-and-not-chosen-were","text":"","title":"Alternatives that were seriously considered and not chosen were"},{"location":"Decision%20Journal/dj%20template/#explain-the-range-of-outcomes","text":"","title":"Explain the range of outcomes"},{"location":"Decision%20Journal/dj%20template/#what-i-expect-to-happen-and-the-actual-probabilities-are","text":"","title":"What I expect to happen and the actual probabilities are"},{"location":"Decision%20Journal/dj%20template/#the-outcome","text":"","title":"The outcome"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/","text":"Common GCP code snippets tool overview list active account name gcloud auth list for i in range(8): print(i) list active project gcloud config list project show default settings for compute engine gcloud compute project-info describe --project qwiklabs-gcp-00-8fce586ced0b set environment variables export PROJECT_ID=qwiklabs-gcp-00-8fce586ced0b export ZONE=us-central1-a create a virtual machine gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone $ZONE view help for compute create command gcloud compute instances create --help view help for gcloud gcloud -h view help for config command gcloud config --help View list of configurations in current environment gcloud config list See what components are available for use gcloud components list GCloud has an interactive mode that can be enables for auto completion. To intstall additional components for auto completion sudo apt-get install google-cloud-sdk enter interactive mode gcloud beta interactive ssh into virtual machine gcloud compute ssh gcelab2 --zone $ZONE change default zone for compute gcloud config set compute/zone us-central1-c Big Query show the details of the shakespeare data in big query bq show bigquery-public-data:samples.shakespeare see a list of commands bq help query perform a query on big query bq query --use_legacy_sql=false \\ 'SELECT word, SUM(word_count) AS count FROM `bigquery-public-data`.samples.shakespeare WHERE word LIKE \"%raisin%\" GROUP BY word' list datasets in current active project bq ls list datasets in specific project bq ls bigquery-public-data: make a dataset in current active project bq mk babynames create a table bq load babynames.names2010 yob2010.txt name:string,gender:string,count:integer query a table bq query \"SELECT name,count FROM babynames.names2010 WHERE gender = 'F' ORDER BY count DESC LIMIT 5\" remove a dataset bq rm -r babynames Google Cloud Storage PROJECT_ID=`gcloud config get-value project` BUCKET=${PROJECT_ID}-bucket create a bucket gsutil mb -c multi_regional gs://${BUCKET} copy local folder \"endpointslambda\" into bucket gsutil -m cp -r endpointslambda gs://${BUCKET} copy item into a folder in the bucket gsutil cp gs://$BUCKET/ada.jpg gs://$BUCKET/image-folder/ download object from bucket gsutil cp -r gs://${BUCKET}/ada.jpg . list items in a bucket gsutil ls gs://${BUCKET}/* rename local file mv endpointslambda/Apache2_0License.txt endpointslambda/old.txt delete local file rm endpointslambda/aeflex-endpoints/app.yaml sync changes with bucket gsutil -m rsync -d -r endpointslambda gs://${BUCKET}/endpointslambda set objects in bucket to be public gsutil -m acl set -R -a public-read gs://${BUCKET} set particular object to be pulic gsutil acl ch -u AllUsers:R gs://$BUCKET/ada.jpg remove public access on an object gsutil acl ch -d AllUsers gs://$BUCKET/ada.jpg upload a file to a bucket and set the storage class to be \"nearline\" gsutil cp -s nearline ghcn/ghcn_on_bq.ipynb gs://${BUCKET} check storage classes of items in bucket gsutil ls -Lr gs://${BUCKET} | more view details for a particular file in the bucket gsutil ls -l gs://$BUCKET/ada.jpg delete all objects in a bucket gsutil rm -rf gs://${BUCKET}/* delete specific object gsutil rm gs://$BUCKET/ada.jpg delete bucket gsutil rb gs://${BUCKET} Google Cloud Functions deploy a cloud function gcloud functions deploy helloWorld \\ --stage-bucket qwiklabs-gcp-00-d68a427ec7ac-bucket \\ --trigger-topic hello_world \\ --runtime nodejs10 For the above there is a file in the current directory called index.js with the follwoing contents /** * Background Cloud Function to be triggered by Pub/Sub. * This function is exported by index.js, and executed when * the trigger topic receives a message. * * @param {object} data The event payload. * @param {object} context The event metadata. */ exports.helloWorld = (data, context) => { const pubSubMessage = data; const name = pubSubMessage.data ? Buffer.from(pubSubMessage.data, 'base64').toString() : \"Hello World\"; console.log(`My Cloud Function: ${name}`); }; Show the status of a cloud function gcloud functions describe helloWorld Read the logs of a cloud function gcloud functions logs read helloWorld Networks create custom network called labnet gcloud compute networks create labnet --subnet-mode=custom create a sub-network gcloud compute networks subnets create labnet-sub \\ --network labnet \\ --region us-central1 \\ --range 10.0.0.0/28 list networks in project gcloud compute networks list view network details gcloud compute networks describe labnet list subnets in all networks in project gcloud compute networks subnets list create firewall rules gcloud compute firewall-rules create labnet-allow-internal \\ --network=labnet \\ --action=ALLOW \\ --rules=icmp,tcp:22 \\ --source-ranges=0.0.0.0/0 view details of a firewall gcloud compute firewall-rules describe labnet-allow-internal IAM Permissions download and run installer to install gcloud curl https://sdk.cloud.google.com | bash restart shell to allow use of the newly installed gcloud tool exec -l $SHELL start configuring gcloud gcloud init Not all components are installed. To see which components are installed run gcloud components list Install beta component gcloud components install beta","title":"Common GCP code snippets"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#common-gcp-code-snippets","text":"tool overview list active account name gcloud auth list for i in range(8): print(i) list active project gcloud config list project show default settings for compute engine gcloud compute project-info describe --project qwiklabs-gcp-00-8fce586ced0b set environment variables export PROJECT_ID=qwiklabs-gcp-00-8fce586ced0b export ZONE=us-central1-a create a virtual machine gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone $ZONE view help for compute create command gcloud compute instances create --help view help for gcloud gcloud -h view help for config command gcloud config --help View list of configurations in current environment gcloud config list See what components are available for use gcloud components list GCloud has an interactive mode that can be enables for auto completion. To intstall additional components for auto completion sudo apt-get install google-cloud-sdk enter interactive mode gcloud beta interactive ssh into virtual machine gcloud compute ssh gcelab2 --zone $ZONE change default zone for compute gcloud config set compute/zone us-central1-c","title":"Common GCP code snippets"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#big-query","text":"show the details of the shakespeare data in big query bq show bigquery-public-data:samples.shakespeare see a list of commands bq help query perform a query on big query bq query --use_legacy_sql=false \\ 'SELECT word, SUM(word_count) AS count FROM `bigquery-public-data`.samples.shakespeare WHERE word LIKE \"%raisin%\" GROUP BY word' list datasets in current active project bq ls list datasets in specific project bq ls bigquery-public-data: make a dataset in current active project bq mk babynames create a table bq load babynames.names2010 yob2010.txt name:string,gender:string,count:integer query a table bq query \"SELECT name,count FROM babynames.names2010 WHERE gender = 'F' ORDER BY count DESC LIMIT 5\" remove a dataset bq rm -r babynames","title":"Big Query"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#google-cloud-storage","text":"PROJECT_ID=`gcloud config get-value project` BUCKET=${PROJECT_ID}-bucket create a bucket gsutil mb -c multi_regional gs://${BUCKET} copy local folder \"endpointslambda\" into bucket gsutil -m cp -r endpointslambda gs://${BUCKET} copy item into a folder in the bucket gsutil cp gs://$BUCKET/ada.jpg gs://$BUCKET/image-folder/ download object from bucket gsutil cp -r gs://${BUCKET}/ada.jpg . list items in a bucket gsutil ls gs://${BUCKET}/* rename local file mv endpointslambda/Apache2_0License.txt endpointslambda/old.txt delete local file rm endpointslambda/aeflex-endpoints/app.yaml sync changes with bucket gsutil -m rsync -d -r endpointslambda gs://${BUCKET}/endpointslambda set objects in bucket to be public gsutil -m acl set -R -a public-read gs://${BUCKET} set particular object to be pulic gsutil acl ch -u AllUsers:R gs://$BUCKET/ada.jpg remove public access on an object gsutil acl ch -d AllUsers gs://$BUCKET/ada.jpg upload a file to a bucket and set the storage class to be \"nearline\" gsutil cp -s nearline ghcn/ghcn_on_bq.ipynb gs://${BUCKET} check storage classes of items in bucket gsutil ls -Lr gs://${BUCKET} | more view details for a particular file in the bucket gsutil ls -l gs://$BUCKET/ada.jpg delete all objects in a bucket gsutil rm -rf gs://${BUCKET}/* delete specific object gsutil rm gs://$BUCKET/ada.jpg delete bucket gsutil rb gs://${BUCKET}","title":"Google Cloud Storage"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#google-cloud-functions","text":"deploy a cloud function gcloud functions deploy helloWorld \\ --stage-bucket qwiklabs-gcp-00-d68a427ec7ac-bucket \\ --trigger-topic hello_world \\ --runtime nodejs10 For the above there is a file in the current directory called index.js with the follwoing contents /** * Background Cloud Function to be triggered by Pub/Sub. * This function is exported by index.js, and executed when * the trigger topic receives a message. * * @param {object} data The event payload. * @param {object} context The event metadata. */ exports.helloWorld = (data, context) => { const pubSubMessage = data; const name = pubSubMessage.data ? Buffer.from(pubSubMessage.data, 'base64').toString() : \"Hello World\"; console.log(`My Cloud Function: ${name}`); }; Show the status of a cloud function gcloud functions describe helloWorld Read the logs of a cloud function gcloud functions logs read helloWorld","title":"Google Cloud Functions"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#networks","text":"create custom network called labnet gcloud compute networks create labnet --subnet-mode=custom create a sub-network gcloud compute networks subnets create labnet-sub \\ --network labnet \\ --region us-central1 \\ --range 10.0.0.0/28 list networks in project gcloud compute networks list view network details gcloud compute networks describe labnet list subnets in all networks in project gcloud compute networks subnets list create firewall rules gcloud compute firewall-rules create labnet-allow-internal \\ --network=labnet \\ --action=ALLOW \\ --rules=icmp,tcp:22 \\ --source-ranges=0.0.0.0/0 view details of a firewall gcloud compute firewall-rules describe labnet-allow-internal","title":"Networks"},{"location":"Google%20Cloud%20Platform/GCP%20code%20snippets/#iam-permissions","text":"download and run installer to install gcloud curl https://sdk.cloud.google.com | bash restart shell to allow use of the newly installed gcloud tool exec -l $SHELL start configuring gcloud gcloud init Not all components are installed. To see which components are installed run gcloud components list Install beta component gcloud components install beta","title":"IAM Permissions"},{"location":"Google%20Cloud%20Platform/Data%20Engineering%20Certificate/de_certificate_misc_notes/","text":"Data Engineering Certificate Misc Notes Professional Data Engineer Sample Questions link Main Google Professional Data Engineer page link Google exam guide. Link","title":"Data Engineering Certificate Misc Notes"},{"location":"Google%20Cloud%20Platform/Data%20Engineering%20Certificate/de_certificate_misc_notes/#data-engineering-certificate-misc-notes","text":"Professional Data Engineer Sample Questions link Main Google Professional Data Engineer page link Google exam guide. Link","title":"Data Engineering Certificate Misc Notes"},{"location":"Google%20Cloud%20Platform/qwiklabs/Baseline%20Data%20ML%20AI%20quest/","text":"AI platform QwikStart another new line","title":"Baseline Data ML AI quest"},{"location":"Google%20Cloud%20Platform/qwiklabs/Data_engineering_quest/","text":"ETL Processing on Google Cloud Using Dataflow and BigQuery lab data_ingestion.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0# # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"`data_ingestion.py` is a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. This example does not do any transformation on the data. \"\"\" import argparse import logging import re import apache_beam as beam from apache_beam.options.pipeline_options import PipelineOptions class DataIngestion: \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date Example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. example output: { 'state': 'KS', 'gender': 'F', 'year': '1923', 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out carriage return, newline and quote characters. values = re.split(\",\", re.sub('\\r\\n', '', re.sub('\"', '', string_input))) row = dict( zip(('state', 'gender', 'year', 'name', 'number', 'created_date'), values)) return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. # Specifically we have the input file to read and the output table to write. # This is the final stage of the pipeline, where we define the destination # of the data. In this case we are writing to BigQuery. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # Useful for developing on a small set of data. default='gs://spls/gsp290/data_files/head_usa_names.csv') # This defaults to the lake dataset in your BigQuery project. You'll have # to create the lake dataset yourself using this command: # bq mk lake parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataIngestion is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataIngestion() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information such as the project ID and # where Dataflow should store temp files. p = beam.Pipeline(options=PipelineOptions(pipeline_args)) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is a # header row. | 'Read from a File' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # This stage of the pipeline translates from a CSV file single row # input as a string, to a dictionary object consumable by BigQuery. # It refers to a function we have written. This function will # be run in parallel on different workers using input from the # previous stage of the pipeline. | 'String To BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the simplest way of defining a schema: # fieldName:fieldType schema='state:STRING,gender:STRING,year:STRING,name:STRING,' 'number:STRING,created_date:STRING', # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() run the script python dataflow_python_examples/data_ingestion.py --project=$PROJECT --region=us-central1 --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --input gs://$PROJECT/data_files/head_usa_names.csv --save_main_session data_transformation.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\" data_transformation.py is a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. This example reads a json schema of the intended output into BigQuery, and transforms the date data to match the format BigQuery expects. \"\"\" import argparse import csv import logging import os import apache_beam as beam from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.io.gcp.bigquery_tools import parse_table_schema_from_json class DataTransformation: \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def __init__(self): dir_path = os.path.dirname(os.path.realpath(__file__)) self.schema_str = '' # Here we read the output schema from a json file. This is used to specify the types # of data we are writing to BigQuery. schema_file = os.path.join(dir_path, 'resources', 'usa_names_year_as_date.json') with open(schema_file) \\ as f: data = f.read() # Wrapping the schema in fields is required for the BigQuery API. self.schema_str = '{\"fields\": ' + data + '}' def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. There are no date format transformations. example output: {'state': 'KS', 'gender': 'F', 'year': '1923-01-01', <- This is the BigQuery date format. 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out return characters and quote characters. schema = parse_table_schema_from_json(self.schema_str) field_map = [f for f in schema.fields] # Use a CSV Reader which can handle quoted strings etc. reader = csv.reader(string_input.split('\\n')) for csv_row in reader: # Our source data only contains year, so default January 1st as the # month and day. month = '01' day = '01' # The year comes from our source data. year = csv_row[2] row = {} i = 0 # Iterate over the values from our csv file, applying any transformation logic. for value in csv_row: # If the schema indicates this field is a date format, we must # transform the date from the source data into a format that # BigQuery can understand. if field_map[i].type == 'DATE': # Format the date to YYYY-MM-DD format which BigQuery # accepts. value = '-'.join((year, month, day)) row[field_map[i].name] = value i += 1 return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. Specifically # we have the input file to load and the output table to write to. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # It is useful for developing on a small set of data default='gs://spls/gsp290/data_files/head_usa_names.csv') # This defaults to the temp dataset in your BigQuery project. You'll have # to create the temp dataset yourself using bq mk temp parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names_transformed') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataTransformation is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataTransformation() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information like where Dataflow should # store temp files, and what the project id is. p = beam.Pipeline(options=PipelineOptions(pipeline_args)) schema = parse_table_schema_from_json(data_ingestion.schema_str) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is a # header row. | 'Read From Text' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # This stage of the pipeline translates from a CSV file single row # input as a string, to a dictionary object consumable by BigQuery. # It refers to a function we have written. This function will # be run in parallel on different workers using input from the # previous stage of the pipeline. | 'String to BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the JSON schema read in from a JSON file. # Specifying the schema allows the API to create the table correctly if it does not yet exist. schema=schema, # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() ddd data_enrichment.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License. # You may obtain a copy of the License at# # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and# limitations under the License. \"\"\" data_enrichment.py demonstrates a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. Along the way, data from BigQuery is read in as a side input and joined in with the primary data from the file. \"\"\" import argparse import csvimport logging import osimport sys import apache_beam as beam from apache_beam.io.gcp import bigquery from apache_beam.io.gcp.bigquery import parse_table_schema_from_json from apache_beam.options.pipeline_options import PipelineOptionsfrom apache_beam.pvalue import AsDict class DataIngestion(object): \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def __init__(self): dir_path = os.path.dirname(os.path.realpath(__file__)) self.schema_str = '' # This is the schema of the destination table in BigQuery. schema_file = os.path.join(dir_path, 'resources', 'usa_names_with_full_state_name.json') with open(schema_file) \\ as f: data = f.read() # Wrapping the schema in fields is required for the BigQuery API. self.schema_str = '{\"fields\": ' + data + '}' def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. There are no date format transformations. example output: {'state': 'KS', 'gender': 'F', 'year': '1923-01-01', <- This is the BigQuery date format. 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out return characters and quote characters. schema = bigquery.parse_table_schema_from_json(self.schema_str) field_map = [f for f in schema.fields] # Use a CSV Reader which can handle quoted strings etc. reader = csv.reader(string_input.split('\\n')) for csv_row in reader: if (sys.version_info.major < 3.0): values = [x.decode('utf8') for x in csv_row] else: values = csv_row # Our source data only contains year, so default January 1st as the # month and day. month = '01' day = '01' # The year comes from our source data. year = values[2] row = {} i = 0 # Iterate over the values from our csv file, applying any transformation logic. for value in values: # If the schema indicates this field is a date format, we must # transform the date from the source data into a format that # BigQuery can understand. if field_map[i].type == 'DATE': # Format the date to YYYY-MM-DD format which BigQuery # accepts. value = '-'.join((year, month, day)) row[field_map[i].name] = value i += 1 return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. Specifically # we have the input file to load and the output table to write to. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # Useful for quickly debugging on a small set of data default='gs://spls/gsp290/data_files/head_usa_names.csv') # The output defaults to the lake dataset in your BigQuery project. You'll have # to create the lake dataset yourself using this command: # bq mk lake parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names_enriched') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataIngestion is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataIngestion() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information like where Dataflow should store # temp files, and what the project id is p = beam.Pipeline(options=PipelineOptions(pipeline_args)) schema = parse_table_schema_from_json(data_ingestion.schema_str) # This function adds in a full state name by looking up the # full name in the short_to_long_name_map. The short_to_long_name_map # comes from a read from BigQuery in the next few lines def add_full_state_name(row, short_to_long_name_map): row['state_full_name'] = short_to_long_name_map[row['state']] return row # This is a second source of data. The source is from BigQuery. # This will come into our pipeline a side input. read_query = \"\"\" SELECT name as state_name, abbreviation as state_abbreviation FROM `qwiklabs-resources.python_dataflow_example.state_abbreviations`\"\"\" state_abbreviations = ( p | 'Read from BigQuery' >> beam.io.Read( beam.io.BigQuerySource(query=read_query, use_standard_sql=True)) # We must create a python tuple of key to value pairs here in order to # use the data as a side input. Dataflow will use the keys to distribute the # work to the correct worker. | 'Abbreviation to Full Name' >> beam.Map( lambda row: (row['state_abbreviation'], row['state_name']))) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is # a header row. | 'Read From Text' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # Translates from the raw string data in the CSV to a dictionary. # The dictionary is a keyed by column names with the values being the values # we want to store in BigQuery. | 'String to BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) # Here we pass in a side input, which is data that comes from outside our # CSV source. The side input contains a map of states to their full name. | 'Join Data' >> beam.Map(add_full_state_name, AsDict( state_abbreviations)) # This is the final stage of the pipeline, where we define the destination # of the data. In this case we are writing to BigQuery. | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the JSON schema read in from a JSON file. # Specifying the schema allows the API to create the table correctly if it does not yet exist. schema=schema, # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() run script python dataflow_python_examples/data_enrichment.py --project=$PROJECT --region=us-central1 --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --input gs://$PROJECT/data_files/head_usa_names.csv --save_main_session data_lake_to_mart.py run script python dataflow_python_examples/data_lake_to_mart.py --worker_disk_type=\"compute.googleapis.com/projects//zones//diskTypes/pd-ssd\" --max_num_workers=4 --project=$PROJECT --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --save_main_session --region=us-central1 Building an IoT Analytics Pipline on Google Cloud Cloud Composer: Copying BigQuery Tables Across Different Locations bq_copy_across_locations.py location # Copyright 2018 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"Example Airflow DAG that performs an export from BQ tables listed in config file to GCS, copies GCS objects across locations (e.g., from US to EU) then imports from GCS to BQ. The DAG imports the gcs_to_gcs operator from plugins and dynamically builds the tasks based on the list of tables. Lastly, the DAG defines a specific application logger to generate logs. This DAG relies on three Airflow variables (https://airflow.apache.org/concepts.html#variables): * table_list_file_path - CSV file listing source and target tables, including Datasets. * gcs_source_bucket - Google Cloud Storage bucket to use for exporting BigQuery tables in source. * gcs_dest_bucket - Google Cloud Storage bucket to use for importing BigQuery tables in destination. See https://cloud.google.com/storage/docs/creating-buckets for creating a bucket. \"\"\" # -------------------------------------------------------------------------------- # Load The Dependencies # -------------------------------------------------------------------------------- import csv import datetime import io import logging from airflow import models from airflow.contrib.operators import bigquery_to_gcs from airflow.contrib.operators import gcs_to_bq from airflow.contrib.operators import gcs_to_gcs from airflow.operators import dummy_operator # -------------------------------------------------------------------------------- # Set default arguments # -------------------------------------------------------------------------------- yesterday = datetime.datetime.now() - datetime.timedelta(days=1) default_args = { 'owner': 'airflow', 'start_date': yesterday, 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': datetime.timedelta(minutes=5), } # -------------------------------------------------------------------------------- # Set variables # -------------------------------------------------------------------------------- # 'table_list_file_path': This variable will contain the location of the master # file. table_list_file_path = models.Variable.get('table_list_file_path') # Source Bucket source_bucket = models.Variable.get('gcs_source_bucket') # Destination Bucket dest_bucket = models.Variable.get('gcs_dest_bucket') # -------------------------------------------------------------------------------- # Set GCP logging # -------------------------------------------------------------------------------- logger = logging.getLogger('bq_copy_us_to_eu_01') # -------------------------------------------------------------------------------- # Functions # -------------------------------------------------------------------------------- def read_table_list(table_list_file): \"\"\" Reads the table list file that will help in creating Airflow tasks in the DAG dynamically. :param table_list_file: (String) The file location of the table list file, e.g. '/home/airflow/framework/table_list.csv' :return table_list: (List) List of tuples containing the source and target tables. \"\"\" table_list = [] logger.info('Reading table_list_file from : %s' % str(table_list_file)) try: with io.open(table_list_file, 'rt', encoding='utf-8') as csv_file: csv_reader = csv.reader(csv_file) next(csv_reader) # skip the headers for row in csv_reader: logger.info(row) table_tuple = { 'table_source': row[0], 'table_dest': row[1] } table_list.append(table_tuple) return table_list except IOError as e: logger.error('Error opening table_list_file %s: ' % str( table_list_file), e) # -------------------------------------------------------------------------------- # Main DAG # -------------------------------------------------------------------------------- # Define a DAG (directed acyclic graph) of tasks. # Any task you create within the context manager is automatically added to the # DAG object. with models.DAG( 'composer_sample_bq_copy_across_locations', default_args=default_args, schedule_interval=None) as dag: start = dummy_operator.DummyOperator( task_id='start', trigger_rule='all_success' ) end = dummy_operator.DummyOperator( task_id='end', trigger_rule='all_success' ) # Get the table list from master file all_records = read_table_list(table_list_file_path) # Loop over each record in the 'all_records' python list to build up # Airflow tasks for record in all_records: logger.info('Generating tasks to transfer table: {}'.format(record)) table_source = record['table_source'] table_dest = record['table_dest'] BQ_to_GCS = bigquery_to_gcs.BigQueryToCloudStorageOperator( # Replace \":\" with valid character for Airflow task task_id='{}_BQ_to_GCS'.format(table_source.replace(\":\", \"_\")), source_project_dataset_table=table_source, destination_cloud_storage_uris=['{}-*.avro'.format( 'gs://' + source_bucket + '/' + table_source)], export_format='AVRO' ) GCS_to_GCS = gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator( # Replace \":\" with valid character for Airflow task task_id='{}_GCS_to_GCS'.format(table_source.replace(\":\", \"_\")), source_bucket=source_bucket, source_object='{}-*.avro'.format(table_source), destination_bucket=dest_bucket, # destination_object='{}-*.avro'.format(table_dest) ) GCS_to_BQ = gcs_to_bq.GoogleCloudStorageToBigQueryOperator( # Replace \":\" with valid character for Airflow task task_id='{}_GCS_to_BQ'.format(table_dest.replace(\":\", \"_\")), bucket=dest_bucket, source_objects=['{}-*.avro'.format(table_source)], destination_project_dataset_table=table_dest, source_format='AVRO', write_disposition='WRITE_TRUNCATE', autodetect=True ) start >> BQ_to_GCS >> GCS_to_GCS >> GCS_to_BQ >> end","title":"ETL Processing on Google Cloud Using Dataflow and BigQuery"},{"location":"Google%20Cloud%20Platform/qwiklabs/Data_engineering_quest/#etl-processing-on-google-cloud-using-dataflow-and-bigquery","text":"lab data_ingestion.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0# # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"`data_ingestion.py` is a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. This example does not do any transformation on the data. \"\"\" import argparse import logging import re import apache_beam as beam from apache_beam.options.pipeline_options import PipelineOptions class DataIngestion: \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date Example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. example output: { 'state': 'KS', 'gender': 'F', 'year': '1923', 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out carriage return, newline and quote characters. values = re.split(\",\", re.sub('\\r\\n', '', re.sub('\"', '', string_input))) row = dict( zip(('state', 'gender', 'year', 'name', 'number', 'created_date'), values)) return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. # Specifically we have the input file to read and the output table to write. # This is the final stage of the pipeline, where we define the destination # of the data. In this case we are writing to BigQuery. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # Useful for developing on a small set of data. default='gs://spls/gsp290/data_files/head_usa_names.csv') # This defaults to the lake dataset in your BigQuery project. You'll have # to create the lake dataset yourself using this command: # bq mk lake parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataIngestion is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataIngestion() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information such as the project ID and # where Dataflow should store temp files. p = beam.Pipeline(options=PipelineOptions(pipeline_args)) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is a # header row. | 'Read from a File' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # This stage of the pipeline translates from a CSV file single row # input as a string, to a dictionary object consumable by BigQuery. # It refers to a function we have written. This function will # be run in parallel on different workers using input from the # previous stage of the pipeline. | 'String To BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the simplest way of defining a schema: # fieldName:fieldType schema='state:STRING,gender:STRING,year:STRING,name:STRING,' 'number:STRING,created_date:STRING', # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() run the script python dataflow_python_examples/data_ingestion.py --project=$PROJECT --region=us-central1 --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --input gs://$PROJECT/data_files/head_usa_names.csv --save_main_session data_transformation.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\" data_transformation.py is a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. This example reads a json schema of the intended output into BigQuery, and transforms the date data to match the format BigQuery expects. \"\"\" import argparse import csv import logging import os import apache_beam as beam from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.io.gcp.bigquery_tools import parse_table_schema_from_json class DataTransformation: \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def __init__(self): dir_path = os.path.dirname(os.path.realpath(__file__)) self.schema_str = '' # Here we read the output schema from a json file. This is used to specify the types # of data we are writing to BigQuery. schema_file = os.path.join(dir_path, 'resources', 'usa_names_year_as_date.json') with open(schema_file) \\ as f: data = f.read() # Wrapping the schema in fields is required for the BigQuery API. self.schema_str = '{\"fields\": ' + data + '}' def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. There are no date format transformations. example output: {'state': 'KS', 'gender': 'F', 'year': '1923-01-01', <- This is the BigQuery date format. 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out return characters and quote characters. schema = parse_table_schema_from_json(self.schema_str) field_map = [f for f in schema.fields] # Use a CSV Reader which can handle quoted strings etc. reader = csv.reader(string_input.split('\\n')) for csv_row in reader: # Our source data only contains year, so default January 1st as the # month and day. month = '01' day = '01' # The year comes from our source data. year = csv_row[2] row = {} i = 0 # Iterate over the values from our csv file, applying any transformation logic. for value in csv_row: # If the schema indicates this field is a date format, we must # transform the date from the source data into a format that # BigQuery can understand. if field_map[i].type == 'DATE': # Format the date to YYYY-MM-DD format which BigQuery # accepts. value = '-'.join((year, month, day)) row[field_map[i].name] = value i += 1 return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. Specifically # we have the input file to load and the output table to write to. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # It is useful for developing on a small set of data default='gs://spls/gsp290/data_files/head_usa_names.csv') # This defaults to the temp dataset in your BigQuery project. You'll have # to create the temp dataset yourself using bq mk temp parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names_transformed') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataTransformation is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataTransformation() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information like where Dataflow should # store temp files, and what the project id is. p = beam.Pipeline(options=PipelineOptions(pipeline_args)) schema = parse_table_schema_from_json(data_ingestion.schema_str) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is a # header row. | 'Read From Text' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # This stage of the pipeline translates from a CSV file single row # input as a string, to a dictionary object consumable by BigQuery. # It refers to a function we have written. This function will # be run in parallel on different workers using input from the # previous stage of the pipeline. | 'String to BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the JSON schema read in from a JSON file. # Specifying the schema allows the API to create the table correctly if it does not yet exist. schema=schema, # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() ddd data_enrichment.py # Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License. # You may obtain a copy of the License at# # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and# limitations under the License. \"\"\" data_enrichment.py demonstrates a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. Along the way, data from BigQuery is read in as a side input and joined in with the primary data from the file. \"\"\" import argparse import csvimport logging import osimport sys import apache_beam as beam from apache_beam.io.gcp import bigquery from apache_beam.io.gcp.bigquery import parse_table_schema_from_json from apache_beam.options.pipeline_options import PipelineOptionsfrom apache_beam.pvalue import AsDict class DataIngestion(object): \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def __init__(self): dir_path = os.path.dirname(os.path.realpath(__file__)) self.schema_str = '' # This is the schema of the destination table in BigQuery. schema_file = os.path.join(dir_path, 'resources', 'usa_names_with_full_state_name.json') with open(schema_file) \\ as f: data = f.read() # Wrapping the schema in fields is required for the BigQuery API. self.schema_str = '{\"fields\": ' + data + '}' def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. There are no date format transformations. example output: {'state': 'KS', 'gender': 'F', 'year': '1923-01-01', <- This is the BigQuery date format. 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out return characters and quote characters. schema = bigquery.parse_table_schema_from_json(self.schema_str) field_map = [f for f in schema.fields] # Use a CSV Reader which can handle quoted strings etc. reader = csv.reader(string_input.split('\\n')) for csv_row in reader: if (sys.version_info.major < 3.0): values = [x.decode('utf8') for x in csv_row] else: values = csv_row # Our source data only contains year, so default January 1st as the # month and day. month = '01' day = '01' # The year comes from our source data. year = values[2] row = {} i = 0 # Iterate over the values from our csv file, applying any transformation logic. for value in values: # If the schema indicates this field is a date format, we must # transform the date from the source data into a format that # BigQuery can understand. if field_map[i].type == 'DATE': # Format the date to YYYY-MM-DD format which BigQuery # accepts. value = '-'.join((year, month, day)) row[field_map[i].name] = value i += 1 return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. Specifically # we have the input file to load and the output table to write to. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # Useful for quickly debugging on a small set of data default='gs://spls/gsp290/data_files/head_usa_names.csv') # The output defaults to the lake dataset in your BigQuery project. You'll have # to create the lake dataset yourself using this command: # bq mk lake parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names_enriched') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataIngestion is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataIngestion() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information like where Dataflow should store # temp files, and what the project id is p = beam.Pipeline(options=PipelineOptions(pipeline_args)) schema = parse_table_schema_from_json(data_ingestion.schema_str) # This function adds in a full state name by looking up the # full name in the short_to_long_name_map. The short_to_long_name_map # comes from a read from BigQuery in the next few lines def add_full_state_name(row, short_to_long_name_map): row['state_full_name'] = short_to_long_name_map[row['state']] return row # This is a second source of data. The source is from BigQuery. # This will come into our pipeline a side input. read_query = \"\"\" SELECT name as state_name, abbreviation as state_abbreviation FROM `qwiklabs-resources.python_dataflow_example.state_abbreviations`\"\"\" state_abbreviations = ( p | 'Read from BigQuery' >> beam.io.Read( beam.io.BigQuerySource(query=read_query, use_standard_sql=True)) # We must create a python tuple of key to value pairs here in order to # use the data as a side input. Dataflow will use the keys to distribute the # work to the correct worker. | 'Abbreviation to Full Name' >> beam.Map( lambda row: (row['state_abbreviation'], row['state_name']))) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is # a header row. | 'Read From Text' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # Translates from the raw string data in the CSV to a dictionary. # The dictionary is a keyed by column names with the values being the values # we want to store in BigQuery. | 'String to BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) # Here we pass in a side input, which is data that comes from outside our # CSV source. The side input contains a map of states to their full name. | 'Join Data' >> beam.Map(add_full_state_name, AsDict( state_abbreviations)) # This is the final stage of the pipeline, where we define the destination # of the data. In this case we are writing to BigQuery. | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the JSON schema read in from a JSON file. # Specifying the schema allows the API to create the table correctly if it does not yet exist. schema=schema, # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run() run script python dataflow_python_examples/data_enrichment.py --project=$PROJECT --region=us-central1 --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --input gs://$PROJECT/data_files/head_usa_names.csv --save_main_session data_lake_to_mart.py run script python dataflow_python_examples/data_lake_to_mart.py --worker_disk_type=\"compute.googleapis.com/projects//zones//diskTypes/pd-ssd\" --max_num_workers=4 --project=$PROJECT --runner=DataflowRunner --staging_location=gs://$PROJECT/test --temp_location gs://$PROJECT/test --save_main_session --region=us-central1","title":"ETL Processing on Google Cloud Using Dataflow and BigQuery"},{"location":"Google%20Cloud%20Platform/qwiklabs/Data_engineering_quest/#building-an-iot-analytics-pipline-on-google-cloud","text":"","title":"Building an IoT Analytics Pipline on Google Cloud"},{"location":"Google%20Cloud%20Platform/qwiklabs/Data_engineering_quest/#cloud-composer-copying-bigquery-tables-across-different-locations","text":"bq_copy_across_locations.py location # Copyright 2018 Google LLC # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"Example Airflow DAG that performs an export from BQ tables listed in config file to GCS, copies GCS objects across locations (e.g., from US to EU) then imports from GCS to BQ. The DAG imports the gcs_to_gcs operator from plugins and dynamically builds the tasks based on the list of tables. Lastly, the DAG defines a specific application logger to generate logs. This DAG relies on three Airflow variables (https://airflow.apache.org/concepts.html#variables): * table_list_file_path - CSV file listing source and target tables, including Datasets. * gcs_source_bucket - Google Cloud Storage bucket to use for exporting BigQuery tables in source. * gcs_dest_bucket - Google Cloud Storage bucket to use for importing BigQuery tables in destination. See https://cloud.google.com/storage/docs/creating-buckets for creating a bucket. \"\"\" # -------------------------------------------------------------------------------- # Load The Dependencies # -------------------------------------------------------------------------------- import csv import datetime import io import logging from airflow import models from airflow.contrib.operators import bigquery_to_gcs from airflow.contrib.operators import gcs_to_bq from airflow.contrib.operators import gcs_to_gcs from airflow.operators import dummy_operator # -------------------------------------------------------------------------------- # Set default arguments # -------------------------------------------------------------------------------- yesterday = datetime.datetime.now() - datetime.timedelta(days=1) default_args = { 'owner': 'airflow', 'start_date': yesterday, 'depends_on_past': False, 'email': [''], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': datetime.timedelta(minutes=5), } # -------------------------------------------------------------------------------- # Set variables # -------------------------------------------------------------------------------- # 'table_list_file_path': This variable will contain the location of the master # file. table_list_file_path = models.Variable.get('table_list_file_path') # Source Bucket source_bucket = models.Variable.get('gcs_source_bucket') # Destination Bucket dest_bucket = models.Variable.get('gcs_dest_bucket') # -------------------------------------------------------------------------------- # Set GCP logging # -------------------------------------------------------------------------------- logger = logging.getLogger('bq_copy_us_to_eu_01') # -------------------------------------------------------------------------------- # Functions # -------------------------------------------------------------------------------- def read_table_list(table_list_file): \"\"\" Reads the table list file that will help in creating Airflow tasks in the DAG dynamically. :param table_list_file: (String) The file location of the table list file, e.g. '/home/airflow/framework/table_list.csv' :return table_list: (List) List of tuples containing the source and target tables. \"\"\" table_list = [] logger.info('Reading table_list_file from : %s' % str(table_list_file)) try: with io.open(table_list_file, 'rt', encoding='utf-8') as csv_file: csv_reader = csv.reader(csv_file) next(csv_reader) # skip the headers for row in csv_reader: logger.info(row) table_tuple = { 'table_source': row[0], 'table_dest': row[1] } table_list.append(table_tuple) return table_list except IOError as e: logger.error('Error opening table_list_file %s: ' % str( table_list_file), e) # -------------------------------------------------------------------------------- # Main DAG # -------------------------------------------------------------------------------- # Define a DAG (directed acyclic graph) of tasks. # Any task you create within the context manager is automatically added to the # DAG object. with models.DAG( 'composer_sample_bq_copy_across_locations', default_args=default_args, schedule_interval=None) as dag: start = dummy_operator.DummyOperator( task_id='start', trigger_rule='all_success' ) end = dummy_operator.DummyOperator( task_id='end', trigger_rule='all_success' ) # Get the table list from master file all_records = read_table_list(table_list_file_path) # Loop over each record in the 'all_records' python list to build up # Airflow tasks for record in all_records: logger.info('Generating tasks to transfer table: {}'.format(record)) table_source = record['table_source'] table_dest = record['table_dest'] BQ_to_GCS = bigquery_to_gcs.BigQueryToCloudStorageOperator( # Replace \":\" with valid character for Airflow task task_id='{}_BQ_to_GCS'.format(table_source.replace(\":\", \"_\")), source_project_dataset_table=table_source, destination_cloud_storage_uris=['{}-*.avro'.format( 'gs://' + source_bucket + '/' + table_source)], export_format='AVRO' ) GCS_to_GCS = gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator( # Replace \":\" with valid character for Airflow task task_id='{}_GCS_to_GCS'.format(table_source.replace(\":\", \"_\")), source_bucket=source_bucket, source_object='{}-*.avro'.format(table_source), destination_bucket=dest_bucket, # destination_object='{}-*.avro'.format(table_dest) ) GCS_to_BQ = gcs_to_bq.GoogleCloudStorageToBigQueryOperator( # Replace \":\" with valid character for Airflow task task_id='{}_GCS_to_BQ'.format(table_dest.replace(\":\", \"_\")), bucket=dest_bucket, source_objects=['{}-*.avro'.format(table_source)], destination_project_dataset_table=table_dest, source_format='AVRO', write_disposition='WRITE_TRUNCATE', autodetect=True ) start >> BQ_to_GCS >> GCS_to_GCS >> GCS_to_BQ >> end","title":"Cloud Composer: Copying BigQuery Tables Across Different Locations"},{"location":"Jobs%20and%20Roles/data_engineer_role_sumathi_team/","text":"Conversation with Sumathi Focus on GCP - Woolworths currently transitioning to have main cloud service being GCP. GCP engineer certification Key tools DE team uses: Google Cloud Storage Cloud composer (key)- Google managed version of Airflow Data flow (key) Cloud functions (key) Big Query (key) (sink 80% of the time) Python GCS -> BigQuery Data engineering team currently getting familiar with GCP Sumathi to review options with Joel. Overall sentiment Should be able to transfer over with: GCP engineer certification Projects demonstrating familiarity with 4 key GCP products","title":"Conversation with Sumathi"},{"location":"Jobs%20and%20Roles/data_engineer_role_sumathi_team/#conversation-with-sumathi","text":"Focus on GCP - Woolworths currently transitioning to have main cloud service being GCP. GCP engineer certification Key tools DE team uses: Google Cloud Storage Cloud composer (key)- Google managed version of Airflow Data flow (key) Cloud functions (key) Big Query (key) (sink 80% of the time) Python GCS -> BigQuery Data engineering team currently getting familiar with GCP Sumathi to review options with Joel. Overall sentiment Should be able to transfer over with: GCP engineer certification Projects demonstrating familiarity with 4 key GCP products","title":"Conversation with Sumathi"},{"location":"Jobs%20and%20Roles/junior_data_engineer_at_woolies01/","text":"From Yury Pitsishin (Head of Machine learning at woolworths rewards) On top of your plan we also usually assess the following skills: - Data Structures & Algorithms (Big O notation), - Linux, - Solution Design of complex high throughput systems (with primary focus on non-functional requirements like security, cost, scalability), - DevOps, - Big Data frameworks, Spark, Scala, - solid experience with one of clouds. I'll be happy to share more details during the catch up. Please find below what we usually require for a Junior role. Minimal Requirements: Bachelor or above degree in Computer Science 3+ years of commercial software development experience Good communication and stakeholder management skills Solid Python and SQL Hands-on Linux Strong problem solving skills Practical experience with Big Data or ML tools, packages and techniques Ideal candidate: Experience of building production grade ML pipelines in GCP or AWS GCP Cloud Engineer or above level of certification Solid experience with streaming technologies like Kafka, Pubsub or Kinesis Questions Data structures and algorithms :: programming language specific? How does understanding BigO notation help solve data engineering problems? What are the kinds of tasks that should be able to be completed using linux What is a high throughput sytem? DevOps is quite broad. any key skills within this? CI/CD, Docker? Spark, Scala - how to get experience with these? Are personal projects enough? Any recommended resources? Who are the typical / common stakeholders? What are typical deliverables? 1 table? Multiple tables? Refresh frequency? within BigQuery? Documentation with field details? Are there any assurances or SLAs about data / pipelines that the team delivers? What do people think is data engineering but actually isn't handled by your team? There was no mention of Airflow. This seems to be a common data engineering","title":"Junior data engineer at woolies01"},{"location":"Linux/Data%20Analytics%20at%20the%20terminal/intro_to_data_analytics_at_terminal/","text":"Introduction to data analytics at the terminal Inspiration: data science at command line Install packages These commands will also install packages on Google Colab sudo pip install csvkit #https://csvkit.readthedocs.io/en/latest/tutorial/1_getting_started.html sudo pip install sqlitebiter #https://sqlitebiter.readthedocs.io/en/latest/pages/usage/index.html sudo pip install sqlite-utils #https://sqlite-utils.datasette.io/en/stable/cli.html sudo apt-get install jq # Install bat wget https://github.com/sharkdp/bat/releases/download/v0.18.0/bat_0.18.0_amd64.deb sudo dpkg -i bat_0.18.0_amd64.deb rm bat_0.18.0_amd64.deb","title":"Introduction to data analytics at the terminal"},{"location":"Linux/Data%20Analytics%20at%20the%20terminal/intro_to_data_analytics_at_terminal/#introduction-to-data-analytics-at-the-terminal","text":"Inspiration: data science at command line","title":"Introduction to data analytics at the terminal"},{"location":"Linux/Data%20Analytics%20at%20the%20terminal/intro_to_data_analytics_at_terminal/#install-packages","text":"These commands will also install packages on Google Colab sudo pip install csvkit #https://csvkit.readthedocs.io/en/latest/tutorial/1_getting_started.html sudo pip install sqlitebiter #https://sqlitebiter.readthedocs.io/en/latest/pages/usage/index.html sudo pip install sqlite-utils #https://sqlite-utils.datasette.io/en/stable/cli.html sudo apt-get install jq # Install bat wget https://github.com/sharkdp/bat/releases/download/v0.18.0/bat_0.18.0_amd64.deb sudo dpkg -i bat_0.18.0_amd64.deb rm bat_0.18.0_amd64.deb","title":"Install packages"},{"location":"Pycon%20US%202021/01%20keynote/","text":"Keynote: Robert Erdmann https://www.rijksmuseum.nl/en/whats-on/exhibitions/operation-night-watch (picture)[http://hyper-resolution.org/view.html?pointer=0.370,0.000&i=Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21]","title":"Keynote: Robert Erdmann"},{"location":"Pycon%20US%202021/01%20keynote/#keynote-robert-erdmann","text":"https://www.rijksmuseum.nl/en/whats-on/exhibitions/operation-night-watch (picture)[http://hyper-resolution.org/view.html?pointer=0.370,0.000&i=Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21]","title":"Keynote: Robert Erdmann"},{"location":"Pycon%20US%202021/02%20creating%20extensible%20workflows/","text":"Creating extensible workflows slides Designing a task API A non trivial workflow system requires task API.","title":"Creating extensible workflows"},{"location":"Pycon%20US%202021/02%20creating%20extensible%20workflows/#creating-extensible-workflows","text":"slides Designing a task API A non trivial workflow system requires task API.","title":"Creating extensible workflows"},{"location":"Pycon%20US%202021/03%20using%20papermill/","text":"What we learned from Papermill to operationalize notebooks parameterisation is the ability to pass values from URLs or elsewhere into the notebook. leverage open source backwards compatibility walk, crawl, run PAPERMILL! with the aim of automating notebooks papermill in azure data studio","title":"What we learned from Papermill to operationalize notebooks"},{"location":"Pycon%20US%202021/03%20using%20papermill/#what-we-learned-from-papermill-to-operationalize-notebooks","text":"parameterisation is the ability to pass values from URLs or elsewhere into the notebook. leverage open source backwards compatibility walk, crawl, run PAPERMILL! with the aim of automating notebooks papermill in azure data studio","title":"What we learned from Papermill to operationalize notebooks"},{"location":"Pycon%20US%202021/04%20documentation%20for%20developers/","text":"Writing Good Documentation for Developers slides Documentation is the user interface for developers and developer tools. Types of documentation Tutorials - step by step guide (studying) Explanations - why (theoretical knowledge) How to guides - real world tasks (working) Reference more details here Reference should be pslit into: Reference Docs API docs - describe code Your user in on a journey that will take them to different areas of documentation. Documentaion is content marketing. Questions are bug reports. Answers are patches.","title":"Writing Good Documentation for Developers"},{"location":"Pycon%20US%202021/04%20documentation%20for%20developers/#writing-good-documentation-for-developers","text":"slides Documentation is the user interface for developers and developer tools. Types of documentation Tutorials - step by step guide (studying) Explanations - why (theoretical knowledge) How to guides - real world tasks (working) Reference more details here Reference should be pslit into: Reference Docs API docs - describe code Your user in on a journey that will take them to different areas of documentation. Documentaion is content marketing. Questions are bug reports. Answers are patches.","title":"Writing Good Documentation for Developers"},{"location":"Pycon%20US%202021/07%20Google%20Serverless%20Application%20Architecture/","text":"Google: Serverless Application Architecture severless workshop slides Serverless code runs on demand. - when there is no work it goes away Serverless code is stateless - when it goes away, so does the memory and file system. Serverless code scales on demand. Most applications need state :: require external storage Events are handled by the platform :: which then invokes your code, once event is handled code stops The application is now a cooperating collection of pieces Google Cloud Serverless (Compute) options: App engine :: useful as a web app backend Cloud Functions :: Good fit for small, focussed event handlers Cloud Run :: Container based, therefore is very flexible Blog post on serverless website that runs pascal here Cloud functions are the shortest path to have live code online. Google Cloud Serverless (State) options: Cloud Firestore Firestore in Datastore Mode Cloud Storage Some event sources: Web requests Updated data Tasks, Scheduler, Pub/Sub Basic Serverless Example To do list One list. One or multiple users use 1 list. Serverless approach: State - list of items Events - Web requests: GET, POST, DELETE, PUT App engine has the ability to use Identity Aware proxy so not everyone can access the app.","title":"Google: Serverless Application Architecture"},{"location":"Pycon%20US%202021/07%20Google%20Serverless%20Application%20Architecture/#google-serverless-application-architecture","text":"severless workshop slides Serverless code runs on demand. - when there is no work it goes away Serverless code is stateless - when it goes away, so does the memory and file system. Serverless code scales on demand. Most applications need state :: require external storage Events are handled by the platform :: which then invokes your code, once event is handled code stops The application is now a cooperating collection of pieces Google Cloud Serverless (Compute) options: App engine :: useful as a web app backend Cloud Functions :: Good fit for small, focussed event handlers Cloud Run :: Container based, therefore is very flexible Blog post on serverless website that runs pascal here Cloud functions are the shortest path to have live code online. Google Cloud Serverless (State) options: Cloud Firestore Firestore in Datastore Mode Cloud Storage Some event sources: Web requests Updated data Tasks, Scheduler, Pub/Sub","title":"Google: Serverless Application Architecture"},{"location":"Pycon%20US%202021/07%20Google%20Serverless%20Application%20Architecture/#basic-serverless-example","text":"","title":"Basic Serverless Example"},{"location":"Pycon%20US%202021/07%20Google%20Serverless%20Application%20Architecture/#to-do-list","text":"One list. One or multiple users use 1 list. Serverless approach: State - list of items Events - Web requests: GET, POST, DELETE, PUT App engine has the ability to use Identity Aware proxy so not everyone can access the app.","title":"To do list"},{"location":"Pycon%20US%202021/08%20Build%20slack%20apps%20fast%20in%20python/","text":"Build Slack apps fast in Python The main developer framework for building slack apps is called bolt Bolt platform examples Slides Permissions are handled with scopes.","title":"Build Slack apps fast in Python"},{"location":"Pycon%20US%202021/08%20Build%20slack%20apps%20fast%20in%20python/#build-slack-apps-fast-in-python","text":"The main developer framework for building slack apps is called bolt Bolt platform examples Slides Permissions are handled with scopes.","title":"Build Slack apps fast in Python"},{"location":"Resources/code_snippets/","text":"Google Cloud platform datafow python script # Copyright 2017 Google Inc. ## Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0# # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. \"\"\"`data_ingestion.py` is a Dataflow pipeline which reads a file and writes its contents to a BigQuery table. This example does not do any transformation on the data. \"\"\" import argparse import logging import re import apache_beam as beam from apache_beam.options.pipeline_options import PipelineOptions class DataIngestion: \"\"\"A helper class which contains the logic to translate the file into a format BigQuery will accept.\"\"\" def parse_method(self, string_input): \"\"\"This method translates a single line of comma separated values to a dictionary which can be loaded into BigQuery. Args: string_input: A comma separated list of values in the form of state_abbreviation,gender,year,name,count_of_babies,dataset_created_date Example string_input: KS,F,1923,Dorothy,654,11/28/2016 Returns: A dict mapping BigQuery column names as keys to the corresponding value parsed from string_input. In this example, the data is not transformed, and remains in the same format as the CSV. example output: { 'state': 'KS', 'gender': 'F', 'year': '1923', 'name': 'Dorothy', 'number': '654', 'created_date': '11/28/2016' } \"\"\" # Strip out carriage return, newline and quote characters. values = re.split(\",\", re.sub('\\r\\n', '', re.sub('\"', '', string_input))) row = dict( zip(('state', 'gender', 'year', 'name', 'number', 'created_date'), values)) return row def run(argv=None): \"\"\"The main function which creates the pipeline and runs it.\"\"\" parser = argparse.ArgumentParser() # Here we add some specific command line arguments we expect. # Specifically we have the input file to read and the output table to write. # This is the final stage of the pipeline, where we define the destination # of the data. In this case we are writing to BigQuery. parser.add_argument( '--input', dest='input', required=False, help='Input file to read. This can be a local file or ' 'a file in a Google Storage Bucket.', # This example file contains a total of only 10 lines. # Useful for developing on a small set of data. default='gs://spls/gsp290/data_files/head_usa_names.csv') # This defaults to the lake dataset in your BigQuery project. You'll have # to create the lake dataset yourself using this command: # bq mk lake parser.add_argument('--output', dest='output', required=False, help='Output BQ table to write results to.', default='lake.usa_names') # Parse arguments from the command line. known_args, pipeline_args = parser.parse_known_args(argv) # DataIngestion is a class we built in this script to hold the logic for # transforming the file into a BigQuery table. data_ingestion = DataIngestion() # Initiate the pipeline using the pipeline arguments passed in from the # command line. This includes information such as the project ID and # where Dataflow should store temp files. p = beam.Pipeline(options=PipelineOptions(pipeline_args)) (p # Read the file. This is the source of the pipeline. All further # processing starts with lines read from the file. We use the input # argument from the command line. We also skip the first line which is a # header row. | 'Read from a File' >> beam.io.ReadFromText(known_args.input, skip_header_lines=1) # This stage of the pipeline translates from a CSV file single row # input as a string, to a dictionary object consumable by BigQuery. # It refers to a function we have written. This function will # be run in parallel on different workers using input from the # previous stage of the pipeline. | 'String To BigQuery Row' >> beam.Map(lambda s: data_ingestion.parse_method(s)) | 'Write to BigQuery' >> beam.io.Write( beam.io.BigQuerySink( # The table name is a required argument for the BigQuery sink. # In this case we use the value passed in from the command line. known_args.output, # Here we use the simplest way of defining a schema: # fieldName:fieldType schema='state:STRING,gender:STRING,year:STRING,name:STRING,' 'number:STRING,created_date:STRING', # Creates the table in BigQuery if it does not yet exist. create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, # Deletes all data in the BigQuery table before writing. write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))) p.run().wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run()","title":"Code snippets"},{"location":"Resources/data_engineering_through_osmosis/","text":"Data Engineering through osmosis Develop and grow understanding of Data Engineering tools and principles by incorportating relevant resources into your media diet. Podcasts Google Cloud Platform Podcast Data Engineering Podcast Talk Python to me Real Python Podcast Python Bytes Test and Code Podcast.init Pybites Social Media Data engineering Subreddit: https://www.reddit.com/r/dataengineering/ Learn python subreddit: https://www.reddit.com/r/learnpython/ Python subreddit: https://www.reddit.com/r/Python/ Mailing Lists","title":"Data Engineering through osmosis"},{"location":"Resources/data_engineering_through_osmosis/#data-engineering-through-osmosis","text":"Develop and grow understanding of Data Engineering tools and principles by incorportating relevant resources into your media diet.","title":"Data Engineering through osmosis"},{"location":"Resources/data_engineering_through_osmosis/#podcasts","text":"Google Cloud Platform Podcast Data Engineering Podcast Talk Python to me Real Python Podcast Python Bytes Test and Code Podcast.init Pybites","title":"Podcasts"},{"location":"Resources/data_engineering_through_osmosis/#social-media","text":"Data engineering Subreddit: https://www.reddit.com/r/dataengineering/ Learn python subreddit: https://www.reddit.com/r/learnpython/ Python subreddit: https://www.reddit.com/r/Python/","title":"Social Media"},{"location":"Resources/data_engineering_through_osmosis/#mailing-lists","text":"","title":"Mailing Lists"},{"location":"Resources/links/","text":"Links A collection of useful links related to the field of data engineering https://awesomedataengineering.com/ Data Engineering Cookbook github repo . GCP certificate data engineering practice exam Become a data engineer on a shoestring","title":"Links"},{"location":"Resources/links/#links","text":"","title":"Links"},{"location":"Resources/links/#a-collection-of-useful-links-related-to-the-field-of-data-engineering","text":"https://awesomedataengineering.com/ Data Engineering Cookbook github repo . GCP certificate data engineering practice exam Become a data engineer on a shoestring","title":"A collection of useful links related to the field of data engineering"},{"location":"Resources/project_ideas/","text":"Review ETL platforms including: - Stitch Data - Panoply - Fivetran - Airbyte Serverless analytics front end tracking. Back end tracking Send data as url parameters to google cloud function (or pub/sub for backend). Write data into BigQuery Use an ETL platform in a project Arsenal dashboard Crypto Dashboard","title":"Project ideas"},{"location":"Resources/videos/","text":"Videos General - the field of data engineering The right path to become a data engineer What is a data engineer Databases Introduction to Database systems Database design course Pipeline Management Apache Airflow - Talk at pycon 2020 Apache Airflow tutorial playlist SQL Intermediate SQL series Inner / outer joins Unions Case statements Having Updating/ Deleting data Aliasing Partition by Advanced SQL series CTE Temp tables String functions Stored procedures Subqueries","title":"Videos"},{"location":"Resources/videos/#videos","text":"","title":"Videos"},{"location":"Resources/videos/#general-the-field-of-data-engineering","text":"","title":"General - the field of data engineering"},{"location":"Resources/videos/#the-right-path-to-become-a-data-engineer","text":"","title":"The right path to become a data engineer"},{"location":"Resources/videos/#what-is-a-data-engineer","text":"","title":"What is a data engineer"},{"location":"Resources/videos/#databases","text":"","title":"Databases"},{"location":"Resources/videos/#introduction-to-database-systems","text":"","title":"Introduction to Database systems"},{"location":"Resources/videos/#database-design-course","text":"","title":"Database design course"},{"location":"Resources/videos/#pipeline-management","text":"","title":"Pipeline Management"},{"location":"Resources/videos/#apache-airflow-talk-at-pycon-2020","text":"","title":"Apache Airflow - Talk at pycon 2020"},{"location":"Resources/videos/#apache-airflow-tutorial-playlist","text":"","title":"Apache Airflow tutorial playlist"},{"location":"Resources/videos/#sql","text":"","title":"SQL"},{"location":"Resources/videos/#intermediate-sql-series","text":"Inner / outer joins Unions Case statements Having Updating/ Deleting data Aliasing Partition by","title":"Intermediate SQL series"},{"location":"Resources/videos/#advanced-sql-series","text":"CTE Temp tables String functions Stored procedures Subqueries","title":"Advanced SQL series"}]}